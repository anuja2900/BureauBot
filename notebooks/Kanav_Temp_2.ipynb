{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "import re\n",
    "import textwrap\n",
    "import time\n",
    "from datetime import date\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import fitz  # PyMuPDF for PDF parsing and generation\n",
    "from google import genai\n",
    "from google.genai import types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List models and their supported methods\n",
    "for m in client.models.list():\n",
    "    caps = [c for c in getattr(m, \"supported_generation_methods\", [])]\n",
    "    print(m.name, caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Configuration and session state\n",
    "#\n",
    "\n",
    "PROJECT_ID = \"adsp-34002-ip07-the-four-musk\"\n",
    "LOCATION = \"us-central1\"\n",
    "MODEL = \"gemini-2.5-pro\"\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key=\"AIzaSyAcI0HYFFuFDZBLGMyzuAuMtUn4GsUH-1o\", vertexai=False\n",
    ")\n",
    "\n",
    "# Upload the form reference once and reuse the resulting handle on each call.\n",
    "upload = client.files.upload(file=\"../data/forms_reference.txt\")\n",
    "REFERENCE_ID = upload.name\n",
    "\n",
    "class SessionState:\n",
    "    \"\"\"A lightweight container for all conversation‑scoped state.\n",
    "\n",
    "    The agent transitions through a number of stages:\n",
    "\n",
    "    - ``select_form``: the agent listens to the user's case description and\n",
    "      suggests an appropriate form.  Clarifying questions may be asked via\n",
    "      the LLM until a canonical form key is determined.\n",
    "    - ``list_fields``: once the form is chosen, the agent gathers a small\n",
    "      number of scoping questions to determine which subset of fields are\n",
    "      applicable to the user.  After collecting the answers, the agent\n",
    "      silently calls the LLM to obtain the list of required fields and asks\n",
    "      the user whether they are ready to begin filling.\n",
    "    - ``fill_individual``: the agent prompts the user for each field one at\n",
    "      a time, validates the reply and stores it.  If a value fails basic\n",
    "      validation, the agent explains the expected format and asks the user to\n",
    "      try again.\n",
    "    - ``complete``: all required fields have been collected.  The agent\n",
    "      synthesises a simple PDF summarising the information and saves it to\n",
    "      ``/home/jupyter/output/<form_key>.pdf``.  The agent informs the user of\n",
    "      the save location.\n",
    "    - ``await_bulk_answers``: a fallback mode retained for compatibility with\n",
    "      the original implementation where the user can provide all answers in\n",
    "      one message.  It is unused in the adjusted workflow but left in place\n",
    "      to honour the original logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.stage: str = \"select_form\"\n",
    "        self.case_info: str = \"\"\n",
    "        self.form_key: str | None = None\n",
    "        self.scoping_questions: List[str] = []\n",
    "        self.scoping_answers_map: Dict[str, str] = {}\n",
    "        self.q_index: int = 0\n",
    "        self.list_phase: str = \"ask_one\"\n",
    "        self.pending_form_question: str | None = None\n",
    "        self.pending_fields: List[str] = []\n",
    "        self.pending_index: int = 0\n",
    "        # Store answers directly keyed by human readable field name\n",
    "        self.answers: Dict[str, str] = {}\n",
    "\n",
    "\n",
    "# Instantiate a global session for simplicity.\n",
    "session = SessionState()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# I/O helpers\n",
    "#\n",
    "def fetch_meta(form_key: str) -> str:\n",
    "    \"\"\"Return the raw JSON metadata for the given form key.\"\"\"\n",
    "    meta_path = pathlib.Path(\"../data/all\") / f\"{form_key}_meta.json\"\n",
    "    return meta_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def parse_pdf(form_key: str) -> str:\n",
    "    \"\"\"Return the concatenated plain text of a PDF form given its key.\"\"\"\n",
    "    pdf_path = pathlib.Path(\"../data/all\") / f\"{form_key}.pdf\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\".join(page.get_text() for page in doc)\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Form key normalisation\n",
    "#\n",
    "ALIAS_MAP: Dict[str, str] = {\n",
    "    # handle common alphanumeric names\n",
    "    \"ar11\": \"uscis_form_ar11\",\n",
    "    \"i246\": \"ice_form_i246\",\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_form_key(text: str) -> str | None:\n",
    "    \"\"\"Normalise free form strings into canonical ``<agency>_form_<code>`` keys.\"\"\"\n",
    "    s = text.lower().strip()\n",
    "    for k, v in ALIAS_MAP.items():\n",
    "        if k in s.replace(\"-\", \"\").replace(\" \", \"\"):\n",
    "            return v\n",
    "    match = re.search(r\"\\b(eoir|cbp|uscis|ice)[\\s_-]*(form)?[\\s_-]*([a-z]?\\d+[a-z]?)\\b\", s)\n",
    "    if not match:\n",
    "        return None\n",
    "    agency = match.group(1)\n",
    "    code = match.group(3)\n",
    "    return f\"{agency}_form_{code}\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# LLM interaction\n",
    "#\n",
    "def call_gemini(system_prompt: str, user_prompt: str, history: List[types.Content] | None = None,\n",
    "                last_n_turns: int = 10) -> str:\n",
    "    \"\"\"Call the Gemini model with a combined system + user prompt.\"\"\"\n",
    "    combined_prompt = f\"{system_prompt.strip()}\\n\\n{user_prompt.strip()}\"\n",
    "    contents: List[types.Content | str] = [upload, \"\\n\\n\", combined_prompt]\n",
    "    if history:\n",
    "        trimmed = history[-(last_n_turns * 2):]\n",
    "        for h in trimmed:\n",
    "            if isinstance(h, types.Content):\n",
    "                contents.append(h)\n",
    "    contents.append(types.Content(role=\"user\", parts=[types.Part(text=combined_prompt)]))\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_output_tokens=5000,\n",
    "        response_modalities=[\"TEXT\"],\n",
    "    )\n",
    "    stream = client.models.generate_content_stream(model=MODEL, contents=contents, config=config)\n",
    "    return \"\".join(chunk.text for chunk in stream).strip()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Prompt templates\n",
    "#\n",
    "SYSTEM_PROMPT = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    You are a U.S. immigration and customs form expert with 30 years of experience.\n",
    "    Be empathetic, confident, and clear.  Always maintain the context.\n",
    "    If the user challenges your choice, explain your reasoning but do not apologise\n",
    "    unless an actual mistake was made; if they are right, update your suggestion.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "PROMPT_SELECT_FORM_CHAT = \"\"\"The user is describing their situation related to immigration/customs.\n",
    "Help them determine which form to file. Ask one clarifying question at a time if needed,\n",
    "provide concise explanations about why you need that information, and when you are confident,\n",
    "recommend the most accurate form and explain briefly why it fits the user's situation.\n",
    "The following reference file contains official descriptions of immigration and customs forms.\n",
    "Use it to decide which form fits the user's situation. Do NOT invent form names or make up a form.\n",
    "If you determine that none of the available forms apply to the user's situation, politely let them know and suggest they consult a professional.\n",
    "\n",
    "When you decide on a form, end with:\n",
    "\"Based on the reasons above, I think you need to fill: {form_key}.\n",
    "Do you want me to fill it out with you? Answer yes to fill the form, or type something else to chat more.\"\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_FIELD_SCOPING_ASK = \"\"\"\n",
    "You are about to help the user fill form {form_key}.\n",
    "Never switch to another form unless the user explicitly asks to change it.\n",
    "Here are the resources:\n",
    "\n",
    "Metadata:\n",
    "{meta_json}\n",
    "\n",
    "Form text:\n",
    "{pdf_text}\n",
    "\n",
    "Questions should be concise, mix of yes/no and short‑answer, and avoid legal jargon.\n",
    "\n",
    "End with:\n",
    "\"Please answer the questions above in free text. After that, I will list the exact fields you need to fill.\"\n",
    "\"\"\"\n",
    "\n",
    "# A prompt used by ``generate_scoping_questions`` to solicit a JSON array of\n",
    "# concise questions from the model.  These questions help the agent determine\n",
    "# which fields of the form the user needs to complete.  The language is kept\n",
    "# simple and the model is instructed to return only a JSON array of strings.\n",
    "PROMPT_FIELD_SCOPING_JSON = \"\"\"\n",
    "You are preparing to gather information from a user in order to complete form {form_key}.\n",
    "Using the metadata and the form text provided, generate a list of short questions\n",
    "that will determine which parts of the form apply to the user.  Each question\n",
    "should require either a yes/no answer or a brief factual response.  Avoid legal\n",
    "jargon and keep the questions clear and concise.  Return **only** a JSON array\n",
    "containing one string per question; do not include any commentary, labels, or\n",
    "markdown formatting.\n",
    "\n",
    "Metadata:\n",
    "{meta_json}\n",
    "\n",
    "Form text:\n",
    "{pdf_text}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_FIELDS_FROM_ANS = \"\"\"\n",
    "Form: {form_key}\n",
    "\n",
    "Metadata:\n",
    "{meta_json}\n",
    "\n",
    "Form text:\n",
    "{pdf_text}\n",
    "\n",
    "User answers to scoping questions:\n",
    "{scoping_answers}\n",
    "\n",
    "From metadata + form text + answers above:\n",
    "1) List ONLY the fields this user needs to fill (exclude agency‑only fields).\n",
    "2) Use human‑readable names (rephrase metadata if needed).\n",
    "3) Keep it as a clean checklist, one per line.\n",
    "4) If the user asked definitions (e.g., “what is mother‑in‑law”), explain briefly.\n",
    "5) Make sure user answers correctly according to the correct form of certain fields (e.g., a Social Security Number has 9 digits, written as XXX‑XX‑XXXX.  If user lists more or less than that, you have to make user do it again until they get it correct).\n",
    "\n",
    " 6) Do not include any greeting, transitional phrase, or commentary. Only list the\n",
    "    checklist of field names and end with the readiness question.\n",
    "\n",
    "End the reply with:\n",
    "\"Are you ready to fill the form?\"\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_LIST_FIELDS_QA = \"\"\"\n",
    "You are helping with form {form_key}.\n",
    "Use the metadata and form text to answer the user's question clearly and briefly.\n",
    "\n",
    "Metadata:\n",
    "{meta_json}\n",
    "\n",
    "Form text:\n",
    "{pdf_text}\n",
    "\n",
    "User message:\n",
    "{user_turn}\n",
    "\n",
    "Answer helpfully. End with: \"Are you ready to fill the form?\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utility functions\n",
    "#\n",
    "def llm_build_pdf_payload(form_key: str, user_block: str, history: List[types.Content] | None = None,\n",
    "                          tries: int = 3) -> Dict[str, str]:\n",
    "    \"\"\"Ask the LLM to map user answers back to metadata field names.\"\"\"\n",
    "    meta_json = fetch_meta(form_key)\n",
    "    pdf_text = parse_pdf(form_key)\n",
    "    base_prompt = textwrap.dedent(\n",
    "        f\"\"\"\n",
    "            You are a CBP/EOIR/USCIS/ICE‑form‑filling expert.\n",
    "            Form metadata:\n",
    "            {meta_json}\n",
    "\n",
    "            Form text:\n",
    "            {pdf_text}\n",
    "\n",
    "            User answers:\n",
    "            {user_block}\n",
    "\n",
    "            TASK\n",
    "            ----\n",
    "            Return ONE JSON object.\n",
    "            • Keys = field \"name\" from metadata that the user clearly answered\n",
    "            • Values = the user’s answer exactly as written\n",
    "            • Omit every un‑answered field\n",
    "            • No markdown, no prose.\n",
    "        \"\"\"\n",
    "    )\n",
    "    for attempt in range(tries):\n",
    "        raw = call_gemini(SYSTEM_PROMPT, base_prompt, history=history)\n",
    "        clean = re.sub(r\"^[`]{3}json|[`]{3}$\", \"\", raw.strip(), flags=re.I | re.M).strip()\n",
    "        try:\n",
    "            return json.loads(clean)\n",
    "        except json.JSONDecodeError:\n",
    "            if attempt == tries - 1:\n",
    "                raise\n",
    "            base_prompt = \"The previous response was invalid JSON. Please output JSON only.\\n\\n\" + base_prompt\n",
    "            time.sleep(0.5)\n",
    "\n",
    "\n",
    "def parse_fields_from_response(response_text: str) -> List[str]:\n",
    "    \"\"\"Extract field names from a checklist returned by the LLM.\"\"\"\n",
    "    fields: List[str] = []\n",
    "    for line in response_text.splitlines():\n",
    "        # stop parsing once the readiness prompt is encountered\n",
    "        if \"Are you ready to fill the form?\" in line:\n",
    "            break\n",
    "        stripped = line.strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "        # remove leading bullets, numbers or whitespace\n",
    "        stripped = re.sub(r\"^[\\-\\*\\d\\.\\)\\s]+\", \"\", stripped).strip()\n",
    "        # Filter out generic headings or prompts that often precede the list\n",
    "        low = stripped.lower()\n",
    "        # skip headings like \"Part 1 – Information About\"\n",
    "        if \"part\" in low and any(x in low for x in [\"information\", \"about\"]):\n",
    "            continue\n",
    "        if \"information about\" in low:\n",
    "            continue\n",
    "        # skip transitional or commentary phrases commonly emitted by the model\n",
    "        if any(keyword in low for keyword in [\"great\", \"here's\", \"continue\", \"fields you need\"]):\n",
    "            continue\n",
    "        # skip lines phrased as questions\n",
    "        if stripped.endswith(\"?\"):\n",
    "            continue\n",
    "        if stripped.lower().startswith(\"what\"):\n",
    "            continue\n",
    "        # skip extremely long lines (heuristic)\n",
    "        if len(stripped) > 80:\n",
    "            continue\n",
    "        fields.append(stripped)\n",
    "    return fields\n",
    "\n",
    "\n",
    "def generate_scoping_questions(form_key: str, history: List[types.Content] | None = None) -> List[str]:\n",
    "    \"\"\"Use the LLM to generate scoping questions for a particular form.\"\"\"\n",
    "    meta_json = fetch_meta(form_key)\n",
    "    pdf_text = parse_pdf(form_key)\n",
    "    # Ask the model for a JSON array of scoping questions.  Using a dedicated\n",
    "    # JSON prompt reduces the likelihood of verbose preambles such as\n",
    "    # \"Part 1 – Information About Yourself\".  The model should return a\n",
    "    # bare JSON array of strings.\n",
    "    prompt = PROMPT_FIELD_SCOPING_JSON.format(\n",
    "        form_key=form_key, meta_json=meta_json, pdf_text=pdf_text\n",
    "    )\n",
    "    raw = call_gemini(SYSTEM_PROMPT, prompt, history=history)\n",
    "    cleaned = re.sub(r\"^\\s*```(?:json)?\\s*|\\s*```\\s*$\", \"\", raw.strip(), flags=re.I | re.M)\n",
    "    try:\n",
    "        data = json.loads(cleaned)\n",
    "        return [str(q).strip() for q in data if str(q).strip()]\n",
    "    except Exception:\n",
    "        # If parsing fails, fall back to splitting by lines and take the first few\n",
    "        lines = [ln.strip(\" -*•\\t\") for ln in cleaned.splitlines() if ln.strip()]\n",
    "        return [ln for ln in lines if len(ln) > 3][:6]\n",
    "\n",
    "\n",
    "def scoping_answers_text(qs: List[str], qa_map: Dict[str, str]) -> str:\n",
    "    \"\"\"Render scoping questions and answers as numbered lines for the LLM.\"\"\"\n",
    "    parts: List[str] = []\n",
    "    for i, q in enumerate(qs, 1):\n",
    "        a = qa_map.get(q, \"\")\n",
    "        if a:\n",
    "            parts.append(f\"{i}. {q}\\n   Answer: {a}\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def validate_answer(field_name: str, answer: str, form_key: str) -> Tuple[bool, str]:\n",
    "    \"\"\"Validate a user's answer for a particular field.\"\"\"\n",
    "    cleaned = answer.strip()\n",
    "    if not cleaned:\n",
    "        return False, f\"Please provide a valid {field_name}.\"\n",
    "    lower_name = field_name.lower()\n",
    "    # Social Security Number\n",
    "    if \"social security\" in lower_name or \"ssn\" in lower_name:\n",
    "        digits = re.sub(r\"\\D\", \"\", cleaned)\n",
    "        if len(digits) == 9:\n",
    "            formatted = f\"{digits[:3]}-{digits[3:5]}-{digits[5:]}\"\n",
    "            return True, formatted\n",
    "        return False, \"Your Social Security Number must have 9 digits, formatted as XXX-XX-XXXX. Please re-enter.\"\n",
    "    # Date fields\n",
    "    if \"date\" in lower_name and any(k in lower_name for k in [\"birth\", \"issue\", \"expiration\", \"expiry\"]):\n",
    "        normalised = re.sub(r\"[.,-]\", \"/\", cleaned)\n",
    "        parts = [p.strip() for p in normalised.split(\"/\")]\n",
    "        if len(parts) == 3:\n",
    "            try:\n",
    "                if len(parts[0]) == 4:\n",
    "                    yr, mo, dy = int(parts[0]), int(parts[1]), int(parts[2])\n",
    "                elif len(parts[2]) == 4:\n",
    "                    mo, dy, yr = int(parts[0]), int(parts[1]), int(parts[2])\n",
    "                else:\n",
    "                    mo, dy, yr = int(parts[0]), int(parts[1]), int(parts[2])\n",
    "                dt = date(yr, mo, dy)\n",
    "                return True, dt.strftime(\"%m/%d/%Y\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        return False, \"Please provide the date in MM/DD/YYYY format. For example: 07/14/1980.\"\n",
    "    # Alien registration / A-number\n",
    "    if any(k in lower_name for k in [\"alien\", \"a-number\", \"a number\", \"anumber\"]):\n",
    "        digits = re.sub(r\"\\D\", \"\", cleaned)\n",
    "        if 7 <= len(digits) <= 9:\n",
    "            return True, (digits if cleaned.lower().startswith(\"a\") else f\"A{digits}\")\n",
    "        return False, \"An A‑Number should be 7 to 9 digits long (e.g., A0123456). Please re-enter.\"\n",
    "    return True, cleaned\n",
    "\n",
    "\n",
    "def save_answers_to_pdf(form_key: str, answers: Dict[str, str], save_dir: pathlib.Path | str = \"/home/jupyter/output\") -> pathlib.Path:\n",
    "    \"\"\"Create a simple PDF summarising the completed fields and save it.\"\"\"\n",
    "    out_dir = pathlib.Path(save_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / f\"{form_key}.pdf\"\n",
    "    doc = fitz.open()\n",
    "    lines = [f\"{field}: {value}\" for field, value in answers.items()]\n",
    "    if not lines:\n",
    "        lines = [\"No data provided.\"]\n",
    "    page = doc.new_page(width=595, height=842)\n",
    "    y = 72\n",
    "    for text_line in lines:\n",
    "        if y > 780:\n",
    "            page = doc.new_page(width=595, height=842)\n",
    "            y = 72\n",
    "        page.insert_text((72, y), text_line, fontsize=12)\n",
    "        y += 18\n",
    "    doc.save(out_path)\n",
    "    doc.close()\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Conversation orchestrator\n",
    "#\n",
    "def on_user_message(user_msg: str, history: List[types.Content]) -> str:\n",
    "    \"\"\"Handle a user message and return the agent's reply.\"\"\"\n",
    "    # Stage 1 — selecting the form\n",
    "    if session.stage == \"select_form\":\n",
    "        session.case_info += \"\\n\" + user_msg\n",
    "        if user_msg.lower() in [\"yes\", \"y\"] and session.form_key:\n",
    "            # transition into the list_fields stage\n",
    "            session.stage = \"list_fields\"\n",
    "            # generate scoping questions via the JSON prompt\n",
    "            session.scoping_questions = generate_scoping_questions(session.form_key, history=history)\n",
    "            session.q_index = 0\n",
    "            session.scoping_answers_map = {}\n",
    "            # If any scoping questions were returned, ask them one by one\n",
    "            if session.scoping_questions:\n",
    "                session.list_phase = \"ask_one\"\n",
    "                return f\"Q1: {session.scoping_questions[0]}\"\n",
    "            # Otherwise, skip directly to deriving the field list and asking to start\n",
    "            meta_json = fetch_meta(session.form_key)\n",
    "            pdf_text = parse_pdf(session.form_key)\n",
    "            # no scoping answers yet\n",
    "            scoping_text = \"\"\n",
    "            prompt = PROMPT_FIELDS_FROM_ANS.format(\n",
    "                form_key=session.form_key,\n",
    "                meta_json=meta_json,\n",
    "                pdf_text=pdf_text,\n",
    "                scoping_answers=scoping_text,\n",
    "            )\n",
    "            # parse the fields from the model's response\n",
    "            reply = call_gemini(SYSTEM_PROMPT, prompt, history=history)\n",
    "            session.pending_fields = parse_fields_from_response(reply)\n",
    "            session.pending_index = 0\n",
    "            session.list_phase = \"done_show\"\n",
    "            return (\n",
    "                f\"I have identified {len(session.pending_fields)} field\"\n",
    "                f\"{'' if len(session.pending_fields) == 1 else 's'} that need to be filled. \"\n",
    "                \"Are you ready to start?\"\n",
    "            )\n",
    "        if session.pending_form_question is not None:\n",
    "            session.case_info += \"\\n\" + user_msg\n",
    "            reply = call_gemini(\n",
    "                SYSTEM_PROMPT,\n",
    "                PROMPT_SELECT_FORM_CHAT.format(form_key=session.form_key or \"___\"),\n",
    "                history=history,\n",
    "            )\n",
    "            key = normalize_form_key(reply)\n",
    "            if key:\n",
    "                session.form_key = key\n",
    "                session.pending_form_question = None\n",
    "                return reply\n",
    "            else:\n",
    "                session.pending_form_question = reply\n",
    "                return reply\n",
    "        reply = call_gemini(\n",
    "            SYSTEM_PROMPT,\n",
    "            PROMPT_SELECT_FORM_CHAT.format(form_key=session.form_key or \"___\"),\n",
    "            history=history,\n",
    "        )\n",
    "        key = normalize_form_key(reply)\n",
    "        if key:\n",
    "            session.form_key = key\n",
    "            return reply\n",
    "        session.pending_form_question = reply\n",
    "        return reply\n",
    "    # Stage 2 — scoping and field list derivation\n",
    "    if session.stage == \"list_fields\":\n",
    "        meta_json = fetch_meta(session.form_key)\n",
    "        pdf_text = parse_pdf(session.form_key)\n",
    "        if session.list_phase == \"ask_one\":\n",
    "            if session.q_index < len(session.scoping_questions):\n",
    "                q = session.scoping_questions[session.q_index]\n",
    "                session.scoping_answers_map[q] = user_msg.strip()\n",
    "                session.q_index += 1\n",
    "            if session.q_index < len(session.scoping_questions):\n",
    "                next_q = session.scoping_questions[session.q_index]\n",
    "                return f\"Q{session.q_index + 1}: {next_q}\"\n",
    "            session.list_phase = \"show\"\n",
    "        if session.list_phase == \"show\":\n",
    "            scoping_text = scoping_answers_text(session.scoping_questions, session.scoping_answers_map)\n",
    "            prompt = PROMPT_FIELDS_FROM_ANS.format(\n",
    "                form_key=session.form_key,\n",
    "                meta_json=meta_json,\n",
    "                pdf_text=pdf_text,\n",
    "                scoping_answers=scoping_text,\n",
    "            )\n",
    "            session.list_phase = \"done_show\"\n",
    "            reply = call_gemini(SYSTEM_PROMPT, prompt, history=history)\n",
    "            session.pending_fields = parse_fields_from_response(reply)\n",
    "            session.pending_index = 0\n",
    "            return (\n",
    "                f\"I have identified {len(session.pending_fields)} field\"\n",
    "                f\"{'' if len(session.pending_fields) == 1 else 's'} that need to be filled. \"\n",
    "                \"Are you ready to start?\"\n",
    "            )\n",
    "        if session.list_phase == \"done_show\":\n",
    "            if user_msg.lower() in [\"yes\", \"y\", \"ready\", \"start\"]:\n",
    "                if session.pending_fields:\n",
    "                    session.stage = \"fill_individual\"\n",
    "                    field = session.pending_fields[session.pending_index]\n",
    "                    return f\"What's your {field}?\"\n",
    "                session.stage = \"await_bulk_answers\"\n",
    "                return \"Great. Please provide all your answers in one message. Use clear labels if possible.\"\n",
    "            prompt = PROMPT_LIST_FIELDS_QA.format(\n",
    "                form_key=session.form_key,\n",
    "                meta_json=meta_json,\n",
    "                pdf_text=pdf_text,\n",
    "                user_turn=user_msg,\n",
    "            )\n",
    "            return call_gemini(SYSTEM_PROMPT, prompt, history=history)\n",
    "    # Stage 3 — filling each field one by one\n",
    "    if session.stage == \"fill_individual\":\n",
    "        if \"?\" in user_msg.strip():\n",
    "            meta_json = fetch_meta(session.form_key)\n",
    "            pdf_text = parse_pdf(session.form_key)\n",
    "            prompt = PROMPT_LIST_FIELDS_QA.format(\n",
    "                form_key=session.form_key,\n",
    "                meta_json=meta_json,\n",
    "                pdf_text=pdf_text,\n",
    "                user_turn=user_msg,\n",
    "            )\n",
    "            answer = call_gemini(SYSTEM_PROMPT, prompt, history=history)\n",
    "            current_field = session.pending_fields[session.pending_index]\n",
    "            return f\"{answer}\\nNow, what's your {current_field}?\"\n",
    "        current_field = session.pending_fields[session.pending_index]\n",
    "        is_valid, payload = validate_answer(current_field, user_msg.strip(), session.form_key)\n",
    "        if not is_valid:\n",
    "            return payload\n",
    "        session.answers[current_field] = payload\n",
    "        session.pending_index += 1\n",
    "        if session.pending_index < len(session.pending_fields):\n",
    "            next_field = session.pending_fields[session.pending_index]\n",
    "            return f\"What's your {next_field}?\"\n",
    "        session.stage = \"complete\"\n",
    "        save_path = save_answers_to_pdf(session.form_key, session.answers)\n",
    "        return (\n",
    "            \"Thank you! I've recorded all your answers and generated a completed form summary. \"\n",
    "            f\"You can find the PDF at: {save_path}.\"\n",
    "        )\n",
    "    # Stage 4 — bulk answer fallback\n",
    "    if session.stage == \"await_bulk_answers\":\n",
    "        payload = llm_build_pdf_payload(session.form_key, user_msg, history=history)\n",
    "        session.answers.update(payload)\n",
    "        session.stage = \"complete\"\n",
    "        save_path = save_answers_to_pdf(session.form_key, session.answers)\n",
    "        return (\n",
    "            \"Thanks! I've parsed your answers and prepared the PDF. \"\n",
    "            f\"You can download it from: {save_path}.\"\n",
    "        )\n",
    "    # Stage 5 — completed\n",
    "    if session.stage == \"complete\":\n",
    "        return \"We've already completed this form. If you have another request, please start a new session.\"\n",
    "    return \"I'm not sure what stage we're in — please describe your case again.\"\n",
    "\n",
    "\n",
    "def chat_with_agent(user_message: str, history: List[types.Content]) -> Tuple[str, List[types.Content]]:\n",
    "    \"\"\"Wrapper that updates the history and delegates to ``on_user_message``.\"\"\"\n",
    "    history.append(types.Content(role=\"user\", parts=[types.Part(text=user_message)]))\n",
    "    backend_reply = on_user_message(user_message, history)\n",
    "    history.append(types.Content(role=\"model\", parts=[types.Part(text=backend_reply)]))\n",
    "    return backend_reply, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------- Testing ----------\n",
    "\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"I am not feeling good about my immigration scene and it's so messed up. blah blah.my green card application got denied by the immigration judge. What form should they fill next?\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"yes\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"ok\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"sure\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Kanav Goyal\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"A92539\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"5239 Apt 2, Hyde Park, Chicago, IL 60615\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"no\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"yes\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Kanav\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"NA\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Goyal\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"A84383243\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"5239 S Kenwood\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"2\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Chicago\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"IL\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"60615\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Attorney\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"123 Main St, Apt 2, New York, NY 11111\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Justice Matters Most\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Client Request\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"9423434\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"today\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"ABCD HHF\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Just Make Money\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"5471 S Ridgewood Ct, Chicago, IL, 60615\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"3122872109\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"1222\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"kannav.goyal@gmail.com\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Yes\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Primary\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"No\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Kanav 21 OCtober Chicago\"\n",
    "reply, history = chat_with_agent(user_input, history)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /home/jupyter/output/eoir_form_27.pdf /home/jupyter/adsp-bureaubot-bucket/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
